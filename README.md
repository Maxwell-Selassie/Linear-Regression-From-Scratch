This project implements Linear Regression completely from scratch in Python, without relying on machine learning libraries like scikit-learn. 
The goal is to understand the mathematics and mechanics behind one of the most fundamental ML algorithms, while practicing clean and reproducible code.

ðŸš€ Features

Implements Linear Regression using:

Gradient Descent optimization

Mean Squared Error (MSE) loss function

Works on any tabular dataset (CSV format)

Includes data preprocessing (normalization & train-test split)

Compares results with scikit-learnâ€™s implementation

ðŸ“‚ Project Structure

â”œâ”€â”€ data/  

â”œâ”€â”€ linear_regression.py 
 
â”œâ”€â”€ notebook.ipynb  

â”œâ”€â”€ poetry.toml      

â””â”€â”€ README.md          

âš¡ Usage

Clone the repo:

git clone https://github.com/yourusername/linear-regression-from-scratch.git
cd linear-regression-from-scratch


Install dependencies:

pip install -r requirements.txt


Run example notebook:

jupyter notebook notebook.ipynb


Or train the model on your own dataset:

python linear_regression.py --data data/your_dataset.csv

âœ… Results

The implementation achieves comparable performance with scikit-learnâ€™s LinearRegression on test datasets.

Visualization of regression line and loss convergence included in notebook.

ðŸŽ¯ Learning Goals

Strengthen understanding of linear regression math

Learn how gradient descent optimization works in practice

Build intuition for how ML libraries implement algorithms internally

ðŸ”® Future Improvements

Add support for polynomial regression

Implement regularization (Ridge & Lasso)

Extend to multivariate datasets
